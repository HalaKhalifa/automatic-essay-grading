{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cG5FePm0MJRb"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "%pip install pip3-autoremove\n",
        "%pip-autoremove torch torchvision torchaudio -y\n",
        "%pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu121\n",
        "%pip install unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5H5T2ZkRTeQ",
        "outputId": "4ae55310-d45e-4e76-8c36-29ac35843b0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU available: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"No GPU available.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fine-tune the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HiAILBncMZ-S",
        "outputId": "af7da1f6-85f9-4d91-871f-dbca3109c9ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2025.5.7: Fast Mistral patching. Transformers: 4.51.3.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "GPU = Tesla T4. Max memory = 14.741 GB.\n",
            "10.854 GB of memory reserved.\n",
            "Starting training with overfitting prevention measures...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 46 | Num Epochs = 46 | Total steps = 500\n",
            "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 4 x 1) = 4\n",
            " \"-____-\"     Trainable parameters = 41,943,040/7,000,000,000 (0.60% trained)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='200' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [200/500 13:56 < 21:08, 0.24 it/s, Epoch 16/46]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.824500</td>\n",
              "      <td>0.670429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.277600</td>\n",
              "      <td>0.232053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.093200</td>\n",
              "      <td>0.176636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.063200</td>\n",
              "      <td>0.192399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.034400</td>\n",
              "      <td>0.173059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.030100</td>\n",
              "      <td>0.167038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>0.027100</td>\n",
              "      <td>0.186531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.025800</td>\n",
              "      <td>0.187339</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [12/12 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Final Training Loss: 0.0258\n",
            "Final Validation Loss: 0.1670\n",
            "Loss Difference (Eval - Train): 0.1412\n",
            "‚úÖ Train/validation loss gap looks reasonable.\n",
            "\n",
            "841.1534 seconds used for training.\n",
            "14.02 minutes used for training.\n",
            "Peak reserved memory = 11.238 GB.\n",
            "Peak reserved memory for training = 0.384 GB.\n",
            "Peak reserved memory % of max memory = 76.236 %.\n",
            "Peak reserved memory for training % of max memory = 2.605 %.\n",
            "\n",
            "==================================================\n",
            "TESTING MODEL WITH CONFIDENCE SCORES\n",
            "==================================================\n",
            "\n",
            "Test Case 1:\n",
            "Score: 1.00\n",
            "Confidence: 1.00\n",
            "\n",
            "Test Case 2:\n",
            "Score: 2.00\n",
            "Confidence: 1.00\n",
            "\n",
            "Model saved to 'essay_grader_regularized' directory\n",
            "\n",
            "‚úÖ Training completed successfully with good generalization!\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import TrainingArguments, EarlyStoppingCallback\n",
        "from trl import SFTTrainer\n",
        "import numpy as np\n",
        "\n",
        "# CONFIGURATION FOR REDUCED OVERFITTING\n",
        "max_seq_length = 4096\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "# Load model\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "# REDUCED LORA CONFIGURATION TO PREVENT OVERFITTING\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,  # REDUCED from 64 - smaller rank prevents overfitting\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],  # REMOVED lm_head\n",
        "    lora_alpha = 16,  # REDUCED from 32 - lower alpha reduces adaptation strength\n",
        "    lora_dropout = 0.1,  # INCREASED from 0.05 - more dropout for regularization\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = True,\n",
        "    loftq_config = None,\n",
        ")\n",
        "\n",
        "# IMPROVED PROMPT TEMPLATE\n",
        "alpaca_prompt = \"\"\"You are an expert essay grader. Grade the following essay based on the provided marking scheme and return only the numerical score.\n",
        "\n",
        "### Marking Scheme:\n",
        "{}\n",
        "\n",
        "### Question:\n",
        "{}\n",
        "\n",
        "### Reference Answer:\n",
        "{}\n",
        "\n",
        "### Student Answer:\n",
        "{}\n",
        "\n",
        "### Score:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    texts = []\n",
        "    for q, ra, sa, ms, score in zip(examples[\"question\"], examples[\"reference_answer\"],\n",
        "                                   examples[\"student_answer\"], examples[\"mark_scheme\"], examples[\"score\"]):\n",
        "        mark_scheme_str = \"\\n\".join([f\"Criterion {i+1}: {k} - {v}\"\n",
        "                                   for i, (k, v) in enumerate(ms.items())])\n",
        "\n",
        "        text = alpaca_prompt.format(\n",
        "            mark_scheme_str,\n",
        "            q,\n",
        "            ra,\n",
        "            sa,\n",
        "            str(score)\n",
        "        ) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "# Load and prepare dataset with LARGER validation split\n",
        "dataset = load_dataset(\"sue888888888888/essay_grading_for_instruction_tuning\", split=\"train\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
        "\n",
        "# LARGER validation split to better detect overfitting\n",
        "dataset_split = dataset.train_test_split(test_size=0.2, seed=3407)\n",
        "train_dataset = dataset_split['train']\n",
        "val_dataset = dataset_split['test']\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Custom metrics to monitor overfitting\"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "    # This is a placeholder - you'd implement actual scoring metrics here\n",
        "    return {\"custom_metric\": 0.0}\n",
        "\n",
        "def main():\n",
        "    # ANTI-OVERFITTING TRAINING CONFIGURATION\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        dataset_text_field=\"text\",\n",
        "        max_seq_length=max_seq_length,\n",
        "        dataset_num_proc=2,\n",
        "        packing=False,\n",
        "        # MORE AGGRESSIVE early stopping\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],  # REDUCED from 3\n",
        "        args=TrainingArguments(\n",
        "            # SMALLER batch sizes and more frequent evaluation\n",
        "            per_device_train_batch_size=1,\n",
        "            per_device_eval_batch_size=1,\n",
        "            gradient_accumulation_steps=4,  # REDUCED from 8\n",
        "\n",
        "            # REDUCED training to prevent overfitting\n",
        "            num_train_epochs=2,  # REDUCED from 3\n",
        "            max_steps=500,  # ADD maximum steps as safety net\n",
        "            learning_rate=2e-5,  # REDUCED from 5e-5 - slower learning\n",
        "            warmup_ratio=0.1,\n",
        "\n",
        "            # Precision and optimization\n",
        "            fp16=not torch.cuda.is_bf16_supported(),\n",
        "            bf16=torch.cuda.is_bf16_supported(),\n",
        "            optim=\"adamw_8bit\",\n",
        "            weight_decay=0.1,  # INCREASED from 0.01 - more regularization\n",
        "            lr_scheduler_type=\"cosine\",\n",
        "\n",
        "            # MORE FREQUENT monitoring to catch overfitting early\n",
        "            logging_steps=5,    # REDUCED from 10\n",
        "            eval_steps=25,      # REDUCED from 50\n",
        "            save_steps=50,      # REDUCED from 100\n",
        "            eval_strategy=\"steps\",\n",
        "            save_strategy=\"steps\",\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"eval_loss\",\n",
        "            greater_is_better=False,\n",
        "\n",
        "            # ADD: Save only best model to prevent using overfit checkpoints\n",
        "            save_total_limit=2,\n",
        "\n",
        "            # Output\n",
        "            output_dir=\"outputs/essay_grader_regularized\",\n",
        "            report_to=\"none\",\n",
        "            seed=3407,\n",
        "\n",
        "            # ADD: Gradient clipping to prevent exploding gradients\n",
        "            max_grad_norm=1.0,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Memory stats\n",
        "    gpu_stats = torch.cuda.get_device_properties(0)\n",
        "    start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "    max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "    print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "    print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
        "\n",
        "    # Train with monitoring\n",
        "    print(\"Starting training with overfitting prevention measures...\")\n",
        "    trainer_stats = trainer.train()\n",
        "\n",
        "        # VALIDATION: Check for overfitting by comparing train vs eval loss\n",
        "    eval_results = trainer.evaluate()\n",
        "    eval_loss = eval_results['eval_loss']\n",
        "\n",
        "    # SAFELY get final training loss\n",
        "    train_loss = None\n",
        "    for record in reversed(trainer.state.log_history):\n",
        "        if \"loss\" in record:\n",
        "            train_loss = record[\"loss\"]\n",
        "            break\n",
        "\n",
        "    if train_loss is None:\n",
        "        print(\"‚ö†Ô∏è Could not retrieve training loss from log history.\")\n",
        "        train_loss = 0.0  # Fallback to avoid crash\n",
        "\n",
        "    print(f\"\\nFinal Training Loss: {train_loss:.4f}\")\n",
        "    print(f\"Final Validation Loss: {eval_loss:.4f}\")\n",
        "    print(f\"Loss Difference (Eval - Train): {eval_loss - train_loss:.4f}\")\n",
        "\n",
        "    if eval_loss - train_loss > 0.5:\n",
        "        print(\"‚ö†Ô∏è  WARNING: Large gap between train and validation loss suggests overfitting!\")\n",
        "    else:\n",
        "        print(\"‚úÖ Train/validation loss gap looks reasonable.\")\n",
        "\n",
        "\n",
        "    # Final memory stats\n",
        "    used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "    used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "    used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "    lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "\n",
        "    print(f\"\\n{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "    print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "    print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "    print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "    print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "    print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n",
        "\n",
        "    # ROBUST INFERENCE FUNCTION\n",
        "    def grade_essay(question, reference_answer, student_answer, mark_scheme_dict, return_confidence=False):\n",
        "        \"\"\"Grade an essay with optional confidence scoring\"\"\"\n",
        "        FastLanguageModel.for_inference(model)\n",
        "\n",
        "        mark_scheme_str = \"\\n\".join([f\"Criterion {i+1}: {k} - {v}\"\n",
        "                                   for i, (k, v) in enumerate(mark_scheme_dict.items())])\n",
        "\n",
        "        prompt = alpaca_prompt.format(\n",
        "            mark_scheme_str,\n",
        "            question,\n",
        "            reference_answer,\n",
        "            student_answer,\n",
        "            \"\"\n",
        "        )\n",
        "\n",
        "        inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "        # Generate multiple samples to check consistency (reduces overfitting effects)\n",
        "        scores = []\n",
        "        for _ in range(3):  # Generate 3 samples\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=10,\n",
        "                    temperature=0.3,    # Slightly higher temperature\n",
        "                    do_sample=True,\n",
        "                    top_p=0.9,\n",
        "                    pad_token_id=tokenizer.eos_token_id\n",
        "                )\n",
        "\n",
        "            decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "            generated_text = decoded.split(\"### Score:\")[-1].strip()\n",
        "\n",
        "            try:\n",
        "                score = float(generated_text.split()[0])\n",
        "                scores.append(score)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        if scores:\n",
        "            avg_score = np.mean(scores)\n",
        "            confidence = 1.0 - (np.std(scores) / (np.mean(scores) + 1e-8))  # Higher std = lower confidence\n",
        "\n",
        "            if return_confidence:\n",
        "                return avg_score, confidence\n",
        "            return avg_score\n",
        "        else:\n",
        "            return \"Error: Could not parse score\"\n",
        "\n",
        "    # TEST WITH MULTIPLE EXAMPLES\n",
        "    test_cases = [\n",
        "        {\n",
        "            \"question\": \"What is photosynthesis?\",\n",
        "            \"reference\": \"Photosynthesis is the process by which green plants make their own food using sunlight, carbon dioxide, and water. The process occurs in the chloroplasts and produces glucose and oxygen as end products.\",\n",
        "            \"student\": \"Photosynthesis is when plants eat sunlight and turn it into food and air.\",\n",
        "            \"mark_scheme\": {\n",
        "                \"Defines photosynthesis correctly\": \"2 points\",\n",
        "                \"Mentions sunlight as energy source\": \"1 point\",\n",
        "                \"Includes CO2 and water as inputs\": \"1 point\",\n",
        "                \"Mentions oxygen/glucose as products\": \"1 point\"\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"Explain Newton's first law of motion.\",\n",
        "            \"reference\": \"Newton's first law states that an object at rest stays at rest and an object in motion stays in motion with the same speed and in the same direction unless acted upon by an unbalanced force.\",\n",
        "            \"student\": \"Things don't move unless you push them, and moving things keep moving unless something stops them.\",\n",
        "            \"mark_scheme\": {\n",
        "                \"States the law correctly\": \"3 points\",\n",
        "                \"Mentions rest and motion\": \"1 point\",\n",
        "                \"Mentions unbalanced force\": \"1 point\"\n",
        "            }\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"TESTING MODEL WITH CONFIDENCE SCORES\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    for i, test_case in enumerate(test_cases, 1):\n",
        "        score, confidence = grade_essay(\n",
        "            test_case[\"question\"],\n",
        "            test_case[\"reference\"],\n",
        "            test_case[\"student\"],\n",
        "            test_case[\"mark_scheme\"],\n",
        "            return_confidence=True\n",
        "        )\n",
        "        print(f\"\\nTest Case {i}:\")\n",
        "        print(f\"Score: {score:.2f}\")\n",
        "        print(f\"Confidence: {confidence:.2f}\")\n",
        "\n",
        "        if confidence < 0.8:\n",
        "            print(\"‚ö†Ô∏è  Low confidence - model may be uncertain or overfitted\")\n",
        "\n",
        "    # Save the model\n",
        "    model.save_pretrained(\"essay_grader_regularized\")\n",
        "    tokenizer.save_pretrained(\"essay_grader_regularized\")\n",
        "    print(f\"\\nModel saved to 'essay_grader_regularized' directory\")\n",
        "\n",
        "    return trainer, eval_loss - train_loss  # Return gap for monitoring\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    trainer, loss_gap = main()\n",
        "\n",
        "    if loss_gap > 0.5:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"OVERFITTING DETECTED - RECOMMENDATIONS:\")\n",
        "        print(\"=\"*60)\n",
        "        print(\"1. Reduce LoRA rank further (try r=8)\")\n",
        "        print(\"2. Increase dropout to 0.2\")\n",
        "        print(\"3. Reduce learning rate to 1e-5\")\n",
        "        print(\"4. Add more training data if possible\")\n",
        "        print(\"5. Consider using smaller model\")\n",
        "        print(\"6. Implement cross-validation\")\n",
        "    else:\n",
        "        print(\"\\n‚úÖ Training completed successfully with good generalization!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test on unseen samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmqeEqogfFOZ",
        "outputId": "5894cbb6-72ec-4f76-8bbe-8beaef4261e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 40 examples from JSON file\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "path = \"generated_essay_grading_samples.json\"\n",
        "\n",
        "# Load JSON into a list of dicts\n",
        "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "    eval_dataset2 = json.load(f)  # Should be a list of dicts\n",
        "\n",
        "print(f\"Loaded {len(eval_dataset2)} examples from JSON file\")\n",
        "\n",
        "# Then call evaluate_model with the loaded data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Byu1wJZYZQ9Q",
        "outputId": "8be3ae31-6b16-48ae-e60a-9e93000d55e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "GPU available: Tesla T4\n",
            "==((====))==  Unsloth 2025.5.7: Fast Mistral patching. Transformers: 4.51.3.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Will load essay_grader_regularized as a legacy tokenizer.\n",
            "Unsloth 2025.5.7 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No test split found. Using 12 examples from train split for evaluation\n",
            "Starting evaluation on 40 samples...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [01:45<00:00,  2.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== EVALUATION METRICS ===\n",
            "accuracy: 0.6250\n",
            "mae: 0.4000\n",
            "mse: 0.4500\n",
            "rmse: 0.6708\n",
            "f1_macro: 0.5746\n",
            "\n",
            "=== SAMPLE RESULTS ===\n",
            "                                          question  true_score  pred_score  \\\n",
            "0    What is a Convolutional Neural Network (CNN)?           1           1   \n",
            "1    What is a Convolutional Neural Network (CNN)?           2           1   \n",
            "2    What is a Convolutional Neural Network (CNN)?           3           2   \n",
            "3    What is a Convolutional Neural Network (CNN)?           4           4   \n",
            "4  What is a Generative Adversarial Network (GAN)?           1           1   \n",
            "5  What is a Generative Adversarial Network (GAN)?           2           1   \n",
            "6  What is a Generative Adversarial Network (GAN)?           3           2   \n",
            "7  What is a Generative Adversarial Network (GAN)?           4           4   \n",
            "8                What is YOLO in machine learning?           1           1   \n",
            "9                What is YOLO in machine learning?           2           1   \n",
            "\n",
            "   correct  \n",
            "0     True  \n",
            "1    False  \n",
            "2    False  \n",
            "3     True  \n",
            "4     True  \n",
            "5    False  \n",
            "6    False  \n",
            "7     True  \n",
            "8     True  \n",
            "9    False  \n",
            "\n",
            "=== SCORE DISTRIBUTION ===\n",
            "True scores distribution:\n",
            "true_score\n",
            "1    10\n",
            "2    10\n",
            "3    10\n",
            "4    10\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Predicted scores distribution:\n",
            "pred_score\n",
            "1    21\n",
            "2     3\n",
            "3     7\n",
            "4     9\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Detailed results saved to 'evaluation_results.csv'\n",
            "\n",
            "=== ERROR ANALYSIS ===\n",
            "Examples with largest errors:\n",
            "\n",
            "Question: What is Clustering in machine learning?\n",
            "True score: 3, Predicted: 1, Error: 2\n",
            "\n",
            "Question: What is a Convolutional Neural Network (CNN)?\n",
            "True score: 2, Predicted: 1, Error: 1\n",
            "\n",
            "Question: What is a Convolutional Neural Network (CNN)?\n",
            "True score: 3, Predicted: 2, Error: 1\n",
            "\n",
            "Question: What is a Generative Adversarial Network (GAN)?\n",
            "True score: 2, Predicted: 1, Error: 1\n",
            "\n",
            "Question: What is a Generative Adversarial Network (GAN)?\n",
            "True score: 3, Predicted: 2, Error: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, mean_absolute_error, mean_squared_error, f1_score\n",
        "import re\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Load fine-tuned model ---\n",
        "max_seq_length = 2048\n",
        "dtype = None  # Auto detection\n",
        "load_in_4bit = True\n",
        "\n",
        "# Check GPU availability\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    print(\"No GPU available. Using CPU (will be slow).\")\n",
        "    device = \"cpu\"\n",
        "\n",
        "# Path to your fine-tuned model (change if needed)\n",
        "model_path = \"essay_grader_regularized\"  # The output_dir from your training script\n",
        "\n",
        "try:\n",
        "    # Load the model - if this fails, may need to specify the exact checkpoint\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = model_path,\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    FastLanguageModel.for_inference(model)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading fine-tuned model: {e}\")\n",
        "    print(\"Falling back to base model (unsloth/mistral-7b-instruct-v0.2-bnb-4bit)\")\n",
        "\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\",\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model)\n",
        "\n",
        "# --- Prepare evaluation dataset ---\n",
        "# Load test split or use a portion of train data if test not available\n",
        "try:\n",
        "    eval_dataset = val_dataset\n",
        "    print(f\"Loaded test split with {len(eval_dataset)} examples\")\n",
        "except:\n",
        "    # If no test split, use a portion of train data\n",
        "    dataset = load_dataset(\"sue888888888888/essay_grading_for_instruction_tuning\", split=\"train\")\n",
        "    # Use 20% of data for evaluation\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    eval_dataset = dataset.select(range(train_size, len(dataset)))\n",
        "    print(f\"No test split found. Using {len(eval_dataset)} examples from train split for evaluation\")\n",
        "\n",
        "# --- Define prompt template ---\n",
        "# Same template as used in training\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes how to grade an essay, paired with an input that provides the grading schema. Write a response that grades essays based on the mark schema provided.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "# --- Evaluation function ---\n",
        "def evaluate_model(model, tokenizer, dataset, num_samples=None):\n",
        "    if num_samples is not None:\n",
        "        if num_samples > len(dataset):\n",
        "            num_samples = len(dataset)\n",
        "        indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
        "        dataset = dataset.select(indices)\n",
        "\n",
        "    results = []\n",
        "    true_scores = []\n",
        "    pred_scores = []\n",
        "\n",
        "    # Process each example in the dataset\n",
        "    for idx, example in enumerate(tqdm(dataset, desc=\"Evaluating\")):\n",
        "        # Format prompt\n",
        "        mark_scheme_str = \"\\n\".join([f\"{k}: {v}\" for k, v in example[\"mark_scheme\"].items()])\n",
        "        instruction = \"Grade this essay based on the following mark scheme:\\n\" + mark_scheme_str\n",
        "        input_text = f\"Question: {example['question']}\\nReference Answer: {example['reference_answer']}\\nStudent Answer: {example['student_answer']}\"\n",
        "\n",
        "        # Generate score\n",
        "        prompt = alpaca_prompt.format(instruction, input_text)\n",
        "\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        # Generate with modest parameters\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=64,\n",
        "            temperature=0.1,  # Low temperature for more deterministic output\n",
        "            top_p=0.9,\n",
        "            do_sample=False,  # Greedy decoding for evaluation\n",
        "            use_cache=True\n",
        "        )\n",
        "\n",
        "        # Decode the output\n",
        "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Extract the generated score - look for the first number in the response\n",
        "        response_part = generated_text.split(\"### Response:\")[-1].strip()\n",
        "\n",
        "        # Extract numeric score using regex\n",
        "        score_match = re.search(r'\\b(\\d+)\\b', response_part)\n",
        "        pred_score = int(score_match.group(1)) if score_match else None\n",
        "\n",
        "        true_score = example[\"score\"]\n",
        "\n",
        "        results.append({\n",
        "            \"index\": idx,\n",
        "            \"question\": example[\"question\"],\n",
        "            \"student_answer\": example[\"student_answer\"][:100] + \"...\",  # Truncate for display\n",
        "            \"true_score\": true_score,\n",
        "            \"pred_score\": pred_score,\n",
        "            \"correct\": pred_score == true_score if pred_score is not None else False,\n",
        "            \"full_response\": response_part\n",
        "        })\n",
        "\n",
        "        if pred_score is not None:\n",
        "            true_scores.append(true_score)\n",
        "            pred_scores.append(pred_score)\n",
        "\n",
        "    # Calculate metrics\n",
        "    metrics = {}\n",
        "    if true_scores and pred_scores:\n",
        "        metrics[\"accuracy\"] = accuracy_score([int(s) for s in true_scores], [int(s) for s in pred_scores])\n",
        "        metrics[\"mae\"] = mean_absolute_error([int(s) for s in true_scores], [int(s) for s in pred_scores])\n",
        "        metrics[\"mse\"] = mean_squared_error([int(s) for s in true_scores], [int(s) for s in pred_scores])\n",
        "        metrics[\"rmse\"] = np.sqrt(metrics[\"mse\"])\n",
        "\n",
        "        # For multi-class F1\n",
        "        metrics[\"f1_macro\"] = f1_score(\n",
        "            [int(s) for s in true_scores],\n",
        "            [int(s) for s in pred_scores],\n",
        "            average='macro'\n",
        "        )\n",
        "\n",
        "    return results, metrics\n",
        "\n",
        "# --- Run evaluation ---\n",
        "# You can adjust the number of samples to evaluate if the dataset is large\n",
        "num_eval_samples = None  # Change to None to evaluate on all samples\n",
        "print(f\"Starting evaluation on {num_eval_samples if num_eval_samples else len(eval_dataset2)} samples...\")\n",
        "results, metrics = evaluate_model(model, tokenizer, eval_dataset2, num_samples=num_eval_samples)\n",
        "\n",
        "# --- Display results ---\n",
        "# Summary metrics\n",
        "print(\"\\n=== EVALUATION METRICS ===\")\n",
        "for metric_name, value in metrics.items():\n",
        "    print(f\"{metric_name}: {value:.4f}\")\n",
        "\n",
        "# Create and display results dataframe\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\n=== SAMPLE RESULTS ===\")\n",
        "print(results_df[[\"question\", \"true_score\", \"pred_score\", \"correct\"]].head(10))\n",
        "\n",
        "# Calculate distribution of scores\n",
        "if results_df[\"pred_score\"].notna().any():\n",
        "    print(\"\\n=== SCORE DISTRIBUTION ===\")\n",
        "    print(\"True scores distribution:\")\n",
        "    print(results_df[\"true_score\"].value_counts().sort_index())\n",
        "    print(\"\\nPredicted scores distribution:\")\n",
        "    print(results_df[\"pred_score\"].value_counts().sort_index())\n",
        "\n",
        "# Save detailed results to CSV\n",
        "results_df.to_csv(\"evaluation_results.csv\", index=False)\n",
        "print(\"\\nDetailed results saved to 'evaluation_results.csv'\")\n",
        "\n",
        "# --- Error Analysis ---\n",
        "if results_df[\"pred_score\"].notna().any():\n",
        "    print(\"\\n=== ERROR ANALYSIS ===\")\n",
        "\n",
        "    # Find examples with largest errors\n",
        "    results_df[\"error\"] = abs(results_df[\"true_score\"] - results_df[\"pred_score\"])\n",
        "    largest_errors = results_df.nlargest(5, \"error\")\n",
        "\n",
        "    print(\"Examples with largest errors:\")\n",
        "    for _, row in largest_errors.iterrows():\n",
        "        print(f\"\\nQuestion: {row['question']}\")\n",
        "        print(f\"True score: {row['true_score']}, Predicted: {row['pred_score']}, Error: {row['error']}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
